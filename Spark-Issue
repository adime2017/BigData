Issue1:
Multiple sources found for csv (org.apache.spark.sql.execution.datasources.csv.CSVFileFormat,com.databricks.spark.csv.DefaultSource15)

Resolution:
-----------
use  csv format as below for this issue
val orderscsv=spark.read.format("org.apache.spark.sql.execution.datasources.csv.CSVFileFormat")
                  .option("header", "false")
                  .option("inferSchema", "true")
                  .load("c:/Users/rv00451128/IdeaProjects/MyFirst/orders_csv - Copy.csv")
                  .toDF("order_id","order_date","order_customer_id","order_status")
 Issue2:
 --------------
 Error:(53, 25) Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and
    Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in
    future releases
 Resolution:
 ---------------
 //Declare case class outside the object scope. Declare case class outside main.

   // case class Order(order_id: Int,order_date:String,order_customer_id:Int,order_status:String)
     val orderds=spark.read.textFile("c:/Users/rv00451128/IdeaProjects/MyFirst/orders.txt")
      .map(o=>{
        val a=o.split(",")
        Order(a(0).toInt,a(1),a(2).toInt,a(3))
      }).as[Order]

Issue3:
--------------------
Seeting up win utils file in Intellije and solving error winutils error:
java.io.IOException: Could not locate executable D:\software\winutils-master\hadoop-2.8.1\bin\winutils.exe in the Hadoop binaries

Download winutilsfile and keep in folder . Example D:/software/hadoop/bin
and set property in your code.
 System.setProperty("hadoop.home.dir", "D:\\software\\winutils-master\\hadoop")
 
 def main(arg: Array[String]){
    System.setProperty("hadoop.home.dir", "D:\\software\\winutils-master\\hadoop")
    val spark=SparkSession
      .builder()
      .appName("Class11")
      //.config("spark.some.config.option", "some-value")
      .master("local")
      .getOrCreate()
      }
    And you are ready to go
    
    
